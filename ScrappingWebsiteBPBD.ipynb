{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from datetime import datetime, timedelta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for date: 01/01/2023\n",
      "Scraping data for date: 02/01/2023\n",
      "Scraping data for date: 03/01/2023\n",
      "Scraping data for date: 04/01/2023\n",
      "Scraping data for date: 05/01/2023\n",
      "Scraping data for date: 06/01/2023\n",
      "Scraping data for date: 07/01/2023\n",
      "Scraping data for date: 08/01/2023\n",
      "Scraping data for date: 09/01/2023\n",
      "Scraping data for date: 10/01/2023\n",
      "Scraping data for date: 11/01/2023\n",
      "Scraping data for date: 12/01/2023\n",
      "Scraping data for date: 13/01/2023\n",
      "Scraping data for date: 14/01/2023\n",
      "Scraping data for date: 15/01/2023\n",
      "Scraping data for date: 16/01/2023\n",
      "Scraping data for date: 17/01/2023\n",
      "Scraping data for date: 18/01/2023\n",
      "Scraping data for date: 19/01/2023\n",
      "Scraping data for date: 20/01/2023\n",
      "Scraping data for date: 21/01/2023\n",
      "Scraping data for date: 22/01/2023\n",
      "Scraping data for date: 23/01/2023\n",
      "Scraping data for date: 24/01/2023\n",
      "Scraping data for date: 25/01/2023\n",
      "Scraping data for date: 26/01/2023\n",
      "Scraping data for date: 27/01/2023\n",
      "Scraping data for date: 28/01/2023\n",
      "Scraping data for date: 29/01/2023\n",
      "Scraping data for date: 30/01/2023\n",
      "Scraping data for date: 31/01/2023\n",
      "Scraping data for date: 01/02/2023\n",
      "Scraping data for date: 02/02/2023\n",
      "Scraping data for date: 03/02/2023\n",
      "Scraping data for date: 04/02/2023\n",
      "Scraping data for date: 05/02/2023\n",
      "Scraping data for date: 06/02/2023\n",
      "Scraping data for date: 07/02/2023\n",
      "Scraping data for date: 08/02/2023\n",
      "Scraping data for date: 09/02/2023\n",
      "Scraping data for date: 10/02/2023\n",
      "Scraping data for date: 11/02/2023\n",
      "Scraping data for date: 12/02/2023\n",
      "Scraping data for date: 13/02/2023\n",
      "Scraping data for date: 14/02/2023\n",
      "Scraping data for date: 15/02/2023\n",
      "Scraping data for date: 16/02/2023\n",
      "Scraping data for date: 17/02/2023\n",
      "Scraping data for date: 18/02/2023\n",
      "Scraping data for date: 19/02/2023\n",
      "Scraping data for date: 20/02/2023\n",
      "Scraping data for date: 21/02/2023\n",
      "Scraping data for date: 22/02/2023\n",
      "Scraping data for date: 23/02/2023\n",
      "Scraping data for date: 24/02/2023\n",
      "Scraping data for date: 25/02/2023\n",
      "Scraping data for date: 26/02/2023\n",
      "Scraping data for date: 27/02/2023\n",
      "Scraping data for date: 28/02/2023\n",
      "Scraping data for date: 01/03/2023\n",
      "Scraping data for date: 02/03/2023\n",
      "Scraping data for date: 03/03/2023\n",
      "Scraping data for date: 04/03/2023\n",
      "Scraping data for date: 05/03/2023\n",
      "Scraping data for date: 06/03/2023\n",
      "Scraping data for date: 07/03/2023\n",
      "Scraping data for date: 08/03/2023\n",
      "Scraping data for date: 09/03/2023\n",
      "Scraping data for date: 10/03/2023\n",
      "Scraping data for date: 11/03/2023\n",
      "Scraping data for date: 12/03/2023\n",
      "Scraping data for date: 13/03/2023\n",
      "Scraping data for date: 14/03/2023\n",
      "Scraping data for date: 15/03/2023\n",
      "Scraping data for date: 16/03/2023\n",
      "Scraping data for date: 17/03/2023\n",
      "Scraping data for date: 18/03/2023\n",
      "Scraping data for date: 19/03/2023\n",
      "Scraping data for date: 20/03/2023\n",
      "Scraping data for date: 21/03/2023\n",
      "Scraping data for date: 22/03/2023\n",
      "Scraping data for date: 23/03/2023\n",
      "Scraping data for date: 24/03/2023\n",
      "Scraping data for date: 25/03/2023\n",
      "Scraping data for date: 26/03/2023\n",
      "Scraping data for date: 27/03/2023\n",
      "Scraping data for date: 28/03/2023\n",
      "Scraping data for date: 29/03/2023\n",
      "Scraping data for date: 30/03/2023\n",
      "Scraping data for date: 31/03/2023\n",
      "Scraping data for date: 01/04/2023\n",
      "Scraping data for date: 02/04/2023\n",
      "Scraping data for date: 03/04/2023\n",
      "Scraping data for date: 04/04/2023\n",
      "Scraping data for date: 05/04/2023\n",
      "Scraping data for date: 06/04/2023\n",
      "Scraping data for date: 07/04/2023\n",
      "Scraping data for date: 08/04/2023\n",
      "Scraping data for date: 09/04/2023\n",
      "Scraping data for date: 10/04/2023\n",
      "Scraping data for date: 11/04/2023\n",
      "Scraping data for date: 12/04/2023\n",
      "Scraping data for date: 13/04/2023\n",
      "Scraping data for date: 14/04/2023\n",
      "Scraping data for date: 15/04/2023\n",
      "Scraping data for date: 16/04/2023\n",
      "Scraping data for date: 17/04/2023\n",
      "Scraping data for date: 18/04/2023\n",
      "Scraping data for date: 19/04/2023\n",
      "Scraping data for date: 20/04/2023\n",
      "Scraping data for date: 21/04/2023\n",
      "Scraping data for date: 22/04/2023\n",
      "Scraping data for date: 23/04/2023\n",
      "Scraping data for date: 24/04/2023\n",
      "Scraping data for date: 25/04/2023\n",
      "Scraping data for date: 26/04/2023\n",
      "Scraping data for date: 27/04/2023\n",
      "Scraping data for date: 28/04/2023\n",
      "Scraping data for date: 29/04/2023\n",
      "Scraping data for date: 30/04/2023\n",
      "Scraping data for date: 01/05/2023\n",
      "Scraping data for date: 02/05/2023\n",
      "Scraping data for date: 03/05/2023\n",
      "Scraping data for date: 04/05/2023\n",
      "Scraping data for date: 05/05/2023\n",
      "Scraping data for date: 06/05/2023\n",
      "Scraping data for date: 07/05/2023\n",
      "Scraping data for date: 08/05/2023\n",
      "Scraping data for date: 09/05/2023\n",
      "Scraping data for date: 10/05/2023\n",
      "Scraping data for date: 11/05/2023\n",
      "Scraping data for date: 12/05/2023\n",
      "Scraping data for date: 13/05/2023\n",
      "Scraping data for date: 14/05/2023\n",
      "Scraping data for date: 15/05/2023\n",
      "Scraping data for date: 16/05/2023\n",
      "Scraping data for date: 17/05/2023\n",
      "Scraping data for date: 18/05/2023\n",
      "Scraping data for date: 19/05/2023\n",
      "Scraping data for date: 20/05/2023\n",
      "Scraping data for date: 21/05/2023\n",
      "Scraping data for date: 22/05/2023\n",
      "Scraping data for date: 23/05/2023\n",
      "Scraping data for date: 24/05/2023\n",
      "Scraping data for date: 25/05/2023\n",
      "Scraping data for date: 26/05/2023\n",
      "Scraping data for date: 27/05/2023\n",
      "Scraping data for date: 28/05/2023\n",
      "Scraping data for date: 29/05/2023\n",
      "Scraping data for date: 30/05/2023\n",
      "Scraping data for date: 31/05/2023\n",
      "Scraping data for date: 01/06/2023\n",
      "Scraping data for date: 02/06/2023\n",
      "Scraping data for date: 03/06/2023\n",
      "Scraping data for date: 04/06/2023\n",
      "Scraping data for date: 05/06/2023\n",
      "Scraping data for date: 06/06/2023\n",
      "Scraping data for date: 07/06/2023\n",
      "Scraping data for date: 08/06/2023\n",
      "Scraping data for date: 09/06/2023\n",
      "Scraping data for date: 10/06/2023\n",
      "Scraping data for date: 11/06/2023\n",
      "Scraping data for date: 12/06/2023\n",
      "Scraping data for date: 13/06/2023\n",
      "Scraping data for date: 14/06/2023\n",
      "Scraping data for date: 15/06/2023\n",
      "Scraping data for date: 16/06/2023\n",
      "Scraping data for date: 17/06/2023\n",
      "Scraping data for date: 18/06/2023\n",
      "Scraping data for date: 19/06/2023\n",
      "Scraping data for date: 20/06/2023\n",
      "Scraping data for date: 21/06/2023\n",
      "Scraping data for date: 22/06/2023\n",
      "Scraping data for date: 23/06/2023\n",
      "Scraping data for date: 24/06/2023\n",
      "Scraping data for date: 25/06/2023\n",
      "Scraping data for date: 26/06/2023\n",
      "Scraping data for date: 27/06/2023\n",
      "Scraping data for date: 28/06/2023\n",
      "Scraping data for date: 29/06/2023\n",
      "Scraping data for date: 30/06/2023\n",
      "Scraping data for date: 01/07/2023\n",
      "Scraping data for date: 02/07/2023\n",
      "Scraping data for date: 03/07/2023\n",
      "Scraping data for date: 04/07/2023\n",
      "Scraping data for date: 05/07/2023\n",
      "Scraping data for date: 06/07/2023\n",
      "Scraping data for date: 07/07/2023\n",
      "Scraping data for date: 08/07/2023\n",
      "Scraping data for date: 09/07/2023\n",
      "Scraping data for date: 10/07/2023\n",
      "Scraping data for date: 11/07/2023\n",
      "Scraping data for date: 12/07/2023\n",
      "Scraping data for date: 13/07/2023\n",
      "Scraping data for date: 14/07/2023\n",
      "Scraping data for date: 15/07/2023\n",
      "Scraping data for date: 16/07/2023\n",
      "Scraping data for date: 17/07/2023\n",
      "Scraping data for date: 18/07/2023\n",
      "Scraping data for date: 19/07/2023\n",
      "Scraping data for date: 20/07/2023\n",
      "Scraping data for date: 21/07/2023\n",
      "Scraping data for date: 22/07/2023\n",
      "Scraping data for date: 23/07/2023\n",
      "Scraping data for date: 24/07/2023\n",
      "Scraping data for date: 25/07/2023\n",
      "Scraping data for date: 26/07/2023\n",
      "Scraping data for date: 27/07/2023\n",
      "Scraping data for date: 28/07/2023\n",
      "Scraping data for date: 29/07/2023\n",
      "Scraping data for date: 30/07/2023\n",
      "Scraping data for date: 31/07/2023\n",
      "Scraping data for date: 01/08/2023\n",
      "Scraping data for date: 02/08/2023\n",
      "Scraping data for date: 03/08/2023\n",
      "Scraping data for date: 04/08/2023\n",
      "Scraping data for date: 05/08/2023\n",
      "Scraping data for date: 06/08/2023\n",
      "Scraping data for date: 07/08/2023\n",
      "Scraping data for date: 08/08/2023\n",
      "Scraping data for date: 09/08/2023\n",
      "Scraping data for date: 10/08/2023\n",
      "Scraping data for date: 11/08/2023\n",
      "Scraping data for date: 12/08/2023\n",
      "Scraping data for date: 13/08/2023\n",
      "Scraping data for date: 14/08/2023\n",
      "Scraping data for date: 15/08/2023\n",
      "Scraping data for date: 16/08/2023\n",
      "Scraping data for date: 17/08/2023\n",
      "Scraping data for date: 18/08/2023\n",
      "Scraping data for date: 19/08/2023\n",
      "Scraping data for date: 20/08/2023\n",
      "Scraping data for date: 21/08/2023\n",
      "Scraping data for date: 22/08/2023\n",
      "Scraping data for date: 23/08/2023\n",
      "Scraping data for date: 24/08/2023\n",
      "Scraping data for date: 25/08/2023\n",
      "Scraping data for date: 26/08/2023\n",
      "Scraping data for date: 27/08/2023\n",
      "Scraping data for date: 28/08/2023\n",
      "Scraping data for date: 29/08/2023\n",
      "Scraping data for date: 30/08/2023\n",
      "Scraping data for date: 31/08/2023\n",
      "Scraping data for date: 01/09/2023\n",
      "Scraping data for date: 02/09/2023\n",
      "Scraping data for date: 03/09/2023\n",
      "Scraping data for date: 04/09/2023\n",
      "Scraping data for date: 05/09/2023\n",
      "Scraping data for date: 06/09/2023\n",
      "Scraping data for date: 07/09/2023\n",
      "Scraping data for date: 08/09/2023\n",
      "Scraping data for date: 09/09/2023\n",
      "Scraping data for date: 10/09/2023\n",
      "Scraping data for date: 11/09/2023\n",
      "Scraping data for date: 12/09/2023\n",
      "Scraping data for date: 13/09/2023\n",
      "Scraping data for date: 14/09/2023\n",
      "Scraping data for date: 15/09/2023\n",
      "Scraping data for date: 16/09/2023\n",
      "Scraping data for date: 17/09/2023\n",
      "Scraping data for date: 18/09/2023\n",
      "Scraping data for date: 19/09/2023\n",
      "Scraping data for date: 20/09/2023\n",
      "Scraping data for date: 21/09/2023\n",
      "Scraping data for date: 22/09/2023\n",
      "Scraping data for date: 23/09/2023\n",
      "Scraping data for date: 24/09/2023\n",
      "Scraping data for date: 25/09/2023\n",
      "Scraping data for date: 26/09/2023\n",
      "Scraping data for date: 27/09/2023\n",
      "Scraping data for date: 28/09/2023\n",
      "Scraping data for date: 29/09/2023\n",
      "Scraping data for date: 30/09/2023\n",
      "Scraping data for date: 01/10/2023\n",
      "Scraping data for date: 02/10/2023\n",
      "Scraping data for date: 03/10/2023\n",
      "Scraping data for date: 04/10/2023\n",
      "Scraping data for date: 05/10/2023\n",
      "Scraping data for date: 06/10/2023\n",
      "Scraping data for date: 07/10/2023\n",
      "Scraping data for date: 08/10/2023\n",
      "Scraping data for date: 09/10/2023\n",
      "Scraping data for date: 10/10/2023\n",
      "Scraping data for date: 11/10/2023\n",
      "Scraping data for date: 12/10/2023\n",
      "Scraping data for date: 13/10/2023\n",
      "Scraping data for date: 14/10/2023\n",
      "Scraping data for date: 15/10/2023\n",
      "Scraping data for date: 16/10/2023\n",
      "Scraping data for date: 17/10/2023\n",
      "Scraping data for date: 18/10/2023\n",
      "Scraping data for date: 19/10/2023\n",
      "Scraping data for date: 20/10/2023\n",
      "Scraping data for date: 21/10/2023\n",
      "Scraping data for date: 22/10/2023\n",
      "Scraping data for date: 23/10/2023\n",
      "Scraping data for date: 24/10/2023\n",
      "Scraping data for date: 25/10/2023\n",
      "Scraping data for date: 26/10/2023\n",
      "Scraping data for date: 27/10/2023\n",
      "Scraping data for date: 28/10/2023\n",
      "Scraping data for date: 29/10/2023\n",
      "Scraping data for date: 30/10/2023\n",
      "Scraping data for date: 31/10/2023\n",
      "Scraping data for date: 01/11/2023\n",
      "Scraping data for date: 02/11/2023\n",
      "Scraping data for date: 03/11/2023\n",
      "Scraping data for date: 04/11/2023\n",
      "Scraping data for date: 05/11/2023\n",
      "Scraping data for date: 06/11/2023\n",
      "Scraping data for date: 07/11/2023\n",
      "Scraping data for date: 08/11/2023\n",
      "Scraping data for date: 09/11/2023\n",
      "Scraping data for date: 10/11/2023\n",
      "Scraping data for date: 11/11/2023\n",
      "Scraping data for date: 12/11/2023\n",
      "Scraping data for date: 13/11/2023\n",
      "Scraping data for date: 14/11/2023\n",
      "Scraping data for date: 15/11/2023\n",
      "Scraping data for date: 16/11/2023\n",
      "Scraping data for date: 17/11/2023\n",
      "Scraping data for date: 18/11/2023\n",
      "Scraping data for date: 19/11/2023\n",
      "Scraping data for date: 20/11/2023\n",
      "Scraping data for date: 21/11/2023\n",
      "Scraping data for date: 22/11/2023\n",
      "Scraping data for date: 23/11/2023\n",
      "Scraping data for date: 24/11/2023\n",
      "Scraping data for date: 25/11/2023\n",
      "Scraping data for date: 26/11/2023\n",
      "Scraping data for date: 27/11/2023\n",
      "Scraping data for date: 28/11/2023\n",
      "Scraping data for date: 29/11/2023\n",
      "Scraping data for date: 30/11/2023\n",
      "Scraping data for date: 01/12/2023\n",
      "Scraping data for date: 02/12/2023\n",
      "Scraping data for date: 03/12/2023\n",
      "Scraping data for date: 04/12/2023\n",
      "Scraping data for date: 05/12/2023\n",
      "Scraping data for date: 06/12/2023\n",
      "Scraping data for date: 07/12/2023\n",
      "Scraping data for date: 08/12/2023\n",
      "Scraping data for date: 09/12/2023\n",
      "Scraping data for date: 10/12/2023\n",
      "Scraping data for date: 11/12/2023\n",
      "Scraping data for date: 12/12/2023\n",
      "Scraping data for date: 13/12/2023\n",
      "Scraping data for date: 14/12/2023\n",
      "Scraping data for date: 15/12/2023\n",
      "Scraping data for date: 16/12/2023\n",
      "Scraping data for date: 17/12/2023\n",
      "Scraping data for date: 18/12/2023\n",
      "Scraping data for date: 19/12/2023\n",
      "Scraping data for date: 20/12/2023\n",
      "Scraping data for date: 21/12/2023\n",
      "Scraping data for date: 22/12/2023\n",
      "Scraping data for date: 23/12/2023\n",
      "Scraping data for date: 24/12/2023\n",
      "Scraping data for date: 25/12/2023\n",
      "Scraping data for date: 26/12/2023\n",
      "Scraping data for date: 27/12/2023\n",
      "Scraping data for date: 28/12/2023\n",
      "Scraping data for date: 29/12/2023\n",
      "Scraping data for date: 30/12/2023\n",
      "Scraping data for date: 31/12/2023\n",
      "Scraping data for date: 01/01/2024\n",
      "Scraping data for date: 02/01/2024\n",
      "Scraping data for date: 03/01/2024\n",
      "Scraping data for date: 04/01/2024\n",
      "Scraping data for date: 05/01/2024\n",
      "Scraping data for date: 06/01/2024\n",
      "Scraping data for date: 07/01/2024\n",
      "Scraping data for date: 08/01/2024\n",
      "Scraping data for date: 09/01/2024\n",
      "Scraping data for date: 10/01/2024\n",
      "Scraping data for date: 11/01/2024\n",
      "Scraping data for date: 12/01/2024\n",
      "Scraping data for date: 13/01/2024\n",
      "Scraping data for date: 14/01/2024\n",
      "Scraping data for date: 15/01/2024\n",
      "Scraping data for date: 16/01/2024\n",
      "Scraping data for date: 17/01/2024\n",
      "Scraping data for date: 18/01/2024\n",
      "Scraping data for date: 19/01/2024\n",
      "Scraping data for date: 20/01/2024\n",
      "Scraping data for date: 21/01/2024\n",
      "Scraping data for date: 22/01/2024\n",
      "Scraping data for date: 23/01/2024\n",
      "Scraping data for date: 24/01/2024\n",
      "Scraping data for date: 25/01/2024\n",
      "Scraping data for date: 26/01/2024\n",
      "Scraping data for date: 27/01/2024\n",
      "Scraping data for date: 28/01/2024\n",
      "Scraping data for date: 29/01/2024\n",
      "Scraping data for date: 30/01/2024\n",
      "Scraping data for date: 31/01/2024\n",
      "Scraping data for date: 01/02/2024\n",
      "Scraping data for date: 02/02/2024\n",
      "Scraping data for date: 03/02/2024\n",
      "Scraping data for date: 04/02/2024\n",
      "Scraping data for date: 05/02/2024\n",
      "Scraping data for date: 06/02/2024\n",
      "Scraping data for date: 07/02/2024\n",
      "Scraping data for date: 08/02/2024\n",
      "Scraping data for date: 09/02/2024\n",
      "Scraping data for date: 10/02/2024\n",
      "Scraping data for date: 11/02/2024\n",
      "Scraping data for date: 12/02/2024\n",
      "Scraping data for date: 13/02/2024\n",
      "Scraping data for date: 14/02/2024\n",
      "Scraping data for date: 15/02/2024\n",
      "Scraping data for date: 16/02/2024\n",
      "Scraping data for date: 17/02/2024\n",
      "Scraping data for date: 18/02/2024\n",
      "Scraping data for date: 19/02/2024\n",
      "Scraping data for date: 20/02/2024\n",
      "Scraping data for date: 21/02/2024\n",
      "Scraping data for date: 22/02/2024\n",
      "Scraping data for date: 23/02/2024\n",
      "Scraping data for date: 24/02/2024\n",
      "Scraping data for date: 25/02/2024\n",
      "Scraping data for date: 26/02/2024\n",
      "Scraping data for date: 27/02/2024\n",
      "Scraping data for date: 28/02/2024\n",
      "Scraping data for date: 29/02/2024\n",
      "Scraping data for date: 01/03/2024\n",
      "Scraping data for date: 02/03/2024\n",
      "Scraping data for date: 03/03/2024\n",
      "Scraping data for date: 04/03/2024\n",
      "Scraping data for date: 05/03/2024\n",
      "Scraping data for date: 06/03/2024\n",
      "Scraping data for date: 07/03/2024\n",
      "Scraping data for date: 08/03/2024\n",
      "Scraping data for date: 09/03/2024\n",
      "Scraping data for date: 10/03/2024\n",
      "Scraping data for date: 11/03/2024\n",
      "Scraping data for date: 12/03/2024\n",
      "Scraping data for date: 13/03/2024\n",
      "Scraping data for date: 14/03/2024\n",
      "Scraping data for date: 15/03/2024\n",
      "Scraping data for date: 16/03/2024\n",
      "Scraping data for date: 17/03/2024\n",
      "Scraping data for date: 18/03/2024\n",
      "Scraping data for date: 19/03/2024\n",
      "Scraping data for date: 20/03/2024\n",
      "Scraping data for date: 21/03/2024\n",
      "Scraping data for date: 22/03/2024\n",
      "Scraping data for date: 23/03/2024\n",
      "Scraping data for date: 24/03/2024\n",
      "Scraping data for date: 25/03/2024\n",
      "Scraping data for date: 26/03/2024\n",
      "Scraping data for date: 27/03/2024\n",
      "Scraping data for date: 28/03/2024\n",
      "Scraping data for date: 29/03/2024\n",
      "Scraping data for date: 30/03/2024\n",
      "Scraping data for date: 31/03/2024\n",
      "Scraping data for date: 01/04/2024\n",
      "Scraping data for date: 02/04/2024\n",
      "Scraping data for date: 03/04/2024\n",
      "Scraping data for date: 04/04/2024\n",
      "Scraping data for date: 05/04/2024\n",
      "Scraping data for date: 06/04/2024\n",
      "Scraping data for date: 07/04/2024\n",
      "Scraping data for date: 08/04/2024\n",
      "Scraping data for date: 09/04/2024\n",
      "Scraping data for date: 10/04/2024\n",
      "Scraping data for date: 11/04/2024\n",
      "Scraping data for date: 12/04/2024\n",
      "Scraping data for date: 13/04/2024\n",
      "Scraping data for date: 14/04/2024\n",
      "Scraping data for date: 15/04/2024\n",
      "Scraping data for date: 16/04/2024\n",
      "Scraping data for date: 17/04/2024\n",
      "Scraping data for date: 18/04/2024\n",
      "Scraping data for date: 19/04/2024\n",
      "Scraping data for date: 20/04/2024\n",
      "Scraping data for date: 21/04/2024\n",
      "Scraping data for date: 22/04/2024\n",
      "Scraping data for date: 23/04/2024\n",
      "Scraping data for date: 24/04/2024\n",
      "Scraping data for date: 25/04/2024\n",
      "Scraping data for date: 26/04/2024\n",
      "Scraping data for date: 27/04/2024\n",
      "Scraping data for date: 28/04/2024\n",
      "Scraping data for date: 29/04/2024\n",
      "Scraping data for date: 30/04/2024\n",
      "Scraping data for date: 01/05/2024\n",
      "Scraping data for date: 02/05/2024\n",
      "Scraping data for date: 03/05/2024\n",
      "Scraping data for date: 04/05/2024\n",
      "Scraping data for date: 05/05/2024\n",
      "Scraping data for date: 06/05/2024\n",
      "Scraping data for date: 07/05/2024\n",
      "Scraping data for date: 08/05/2024\n",
      "Scraping data for date: 09/05/2024\n",
      "Scraping data for date: 10/05/2024\n",
      "Scraping data for date: 11/05/2024\n",
      "Scraping data for date: 12/05/2024\n",
      "Scraping data for date: 13/05/2024\n",
      "Scraping data for date: 14/05/2024\n",
      "Scraping data for date: 15/05/2024\n",
      "Scraping data for date: 16/05/2024\n",
      "Scraping data for date: 17/05/2024\n",
      "Scraping data for date: 18/05/2024\n",
      "Scraping data for date: 19/05/2024\n",
      "Scraping data for date: 20/05/2024\n",
      "Scraping data for date: 21/05/2024\n",
      "Scraping data for date: 22/05/2024\n",
      "Scraping data for date: 23/05/2024\n",
      "Scraping data for date: 24/05/2024\n",
      "Scraping data for date: 25/05/2024\n",
      "Scraping data for date: 26/05/2024\n",
      "Scraping data for date: 27/05/2024\n",
      "Scraping data for date: 28/05/2024\n",
      "Scraping data for date: 29/05/2024\n",
      "Scraping data for date: 30/05/2024\n",
      "Scraping data for date: 31/05/2024\n",
      "Scraping data for date: 01/06/2024\n",
      "Scraping data for date: 02/06/2024\n",
      "Scraping data for date: 03/06/2024\n",
      "Scraping data for date: 04/06/2024\n",
      "Scraping data for date: 05/06/2024\n",
      "Scraping data for date: 06/06/2024\n",
      "Scraping data for date: 07/06/2024\n",
      "Scraping data for date: 08/06/2024\n",
      "Scraping data for date: 09/06/2024\n",
      "Scraping data for date: 10/06/2024\n",
      "Scraping data for date: 11/06/2024\n",
      "Scraping data for date: 12/06/2024\n",
      "Scraping data for date: 13/06/2024\n",
      "Scraping data for date: 14/06/2024\n",
      "Scraping data for date: 15/06/2024\n",
      "Scraping data for date: 16/06/2024\n",
      "Scraping data for date: 17/06/2024\n",
      "Scraping data for date: 18/06/2024\n",
      "Scraping data for date: 19/06/2024\n",
      "Scraping data for date: 20/06/2024\n",
      "Scraping data for date: 21/06/2024\n",
      "Scraping data for date: 22/06/2024\n",
      "Scraping data for date: 23/06/2024\n",
      "Scraping data for date: 24/06/2024\n",
      "Scraping data for date: 25/06/2024\n",
      "Scraping data for date: 26/06/2024\n",
      "Scraping data for date: 27/06/2024\n",
      "Scraping data for date: 28/06/2024\n",
      "Scraping data for date: 29/06/2024\n",
      "Scraping data for date: 30/06/2024\n",
      "[['Pintu Air', '23:00', '22:00', '21:00', '20:00', '19:00', '18:00', '17:00', '16:00', '15:00', '14:00', '13:00', '12:00', '11:00', '10:00', '09:00', '08:00', '07:00', '06:00', '05:00', '04:00', '03:00', '02:00', '01:00', '00:00', ' ', '01/01/2023'], ['Bendung Katulampa', '40 cm (MT)', '40 cm (MT)', '40 cm (MT)', '40 cm (MT)', '40 cm (MT)', '40 cm (T)', '40 cm (T)', '40 cm (T)', '40 cm (T)', '40 cm (MT)', '40 cm (MT)', '50 cm (MT)', '50 cm (MT)', '60 cm (M)', '60 cm (M)', '50 cm (M)', '50 cm (G)', '50 cm (H)', '40 cm (MT)', '40 cm (MT)', '40 cm (MT)', '40 cm (MT)', '40 cm (MT)', '40 cm (MT)', ' ', '01/01/2023'], ['Pos Depok', '115 cm (T)', '115 cm (T)', '115 cm (T)', '120 cm (T)', '120 cm (T)', '120 cm (T)', '125 cm (T)', '125 cm (MT)', '135 cm (MT)', '140 cm (MT)', '150 cm (G)', '130 cm (M)', '130 cm (M)', '130 cm (M)', '120 cm (M)', '115 cm (M)', '120 cm (G)', '110 cm (M)', '115 cm (M)', '115 cm (G)', '115 cm (M)', '120 cm (MT)', '120 cm (M)', '125 cm (T)', ' ', '01/01/2023'], ['Manggarai BKB', '710 cm (T)', '715 cm (T)', '715 cm (T)', '710 cm (T)', '700 cm (T)', '695 cm (T)', '690 cm (T)', '690 cm (T)', '690 cm (M)', '690 cm (M)', '685 cm (MT)', '685 cm (M)', '690 cm (M)', '700 cm (G)', '710 cm (G)', '700 cm (H)', '700 cm (G)', '705 cm (M)', '700 cm (G)', '700 cm (MT)', '700 cm (MT)', '700 cm (MT)', '700 cm (G)', '695 cm (T)', ' ', '01/01/2023'], ['PA. Karet', '350 cm (T)', '350 cm (T)', '350 cm (T)', '340 cm (T)', '340 cm (T)', '340 cm (T)', '340 cm (T)', '340 cm (T)', '340 cm (T)', '340 cm (G)', '340 cm (M)', '350 cm (M)', '370 cm (M)', '380 cm (M)', '370 cm (G)', '350 cm (H)', '350 cm (M)', '350 cm (M)', '350 cm (G)', '340 cm (M)', '340 cm (MT)', '340 cm (M)', '340 cm (G)', '340 cm (T)', ' ', '01/01/2023']]\n"
     ]
    }
   ],
   "source": [
    "def get_table_data(driver, date):\n",
    "    WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.TAG_NAME, 'table')))\n",
    "    table = driver.find_element(By.TAG_NAME, 'table')\n",
    "    rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "    data = []\n",
    "    for row in rows[1:]:  # Skipping header row\n",
    "        cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "        cols = [col.text for col in cols]\n",
    "        cols.append(date)  # Adding the date as the last column\n",
    "        data.append(cols)\n",
    "    return data\n",
    "\n",
    "\n",
    "def set_date_filter(driver, date):\n",
    "    date_input = driver.find_element(By.ID, 'datetma')\n",
    "    date_input.clear()\n",
    "    date_input.send_keys(date)\n",
    "    date_input.send_keys(Keys.RETURN)\n",
    "    filter_button = driver.find_element(By.CLASS_NAME, 'mybutton1')\n",
    "    filter_button.click()\n",
    "\n",
    "def scrape_data_for_date(driver, date):\n",
    "    try:\n",
    "        set_date_filter(driver, date)\n",
    "        data = get_table_data(driver,date)\n",
    "        return data\n",
    "    except TimeoutException as e:\n",
    "        print(f\"TimeoutException: The table was not found for date {date}.\")\n",
    "        return []\n",
    "\n",
    "def scrape_data_with_selenium(start_date, end_date):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(\"https://bpbd.jakarta.go.id/waterlevel\")\n",
    "    \n",
    "    all_data = []\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        formatted_date = current_date.strftime(\"%d/%m/%Y\")\n",
    "        print(f\"Scraping data for date: {formatted_date}\")\n",
    "        data = scrape_data_for_date(driver, formatted_date)\n",
    "        all_data.extend(data)\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    driver.quit()\n",
    "    return all_data\n",
    "\n",
    "# Define the start and end dates for a year\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2024, 6, 30)\n",
    "\n",
    "# Scrape the data\n",
    "all_data = scrape_data_with_selenium(start_date, end_date)\n",
    "\n",
    "# Print the first few rows to understand the structure\n",
    "print(all_data[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraping completed and saved to 'water_level_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Adjust the column names based on the actual table structure\n",
    "columns = [\n",
    "    'Pintu Air', '23:00', '22:00', '21:00', '20:00', '19:00',\n",
    "    '18:00', '17:00', '16:00', '15:00', '14:00', '13:00',\n",
    "    '12:00', '11:00', '10:00', '09:00', '08:00', '07:00',\n",
    "    '06:00', '05:00', '04:00', '03:00', '02:00', '01:00','00:00','add','add','add','add','add','add','Tanggal'\n",
    "]  # Adjust these names as necessary\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(all_data, columns=columns)\n",
    "\n",
    "# Save to CSV or any other format\n",
    "df.to_excel('Scrap2324.xlsx', index=False)\n",
    "print(\"Data scraping completed and saved to 'water_level_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghilangkan baris 2, 15, 28, dst.\n",
    "df = df.drop(index=df.index[0::13])\n",
    "\n",
    "df.to_excel('TMA20232024.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File telah disimpan di: Reformatted_TMA2023.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# # Load the Excel file\n",
    "# file_path = 'TMA2023.xlsx'  # ganti dengan path file yang sesuai\n",
    "# df = pd.read_excel(file_path)\n",
    "\n",
    "# Prepare the date range and time values\n",
    "dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\n",
    "times = [f'{hour:02d}:00' for hour in range(24)]\n",
    "\n",
    "# Prepare the new DataFrame structure\n",
    "num_dates = len(dates)\n",
    "num_times = len(times)\n",
    "num_pintu_air = 12\n",
    "total_rows = num_dates * num_times\n",
    "\n",
    "# Initialize the new DataFrame\n",
    "columns = ['Tanggal', 'Waktu'] + [f'Pintu Air {i+1}' for i in range(num_pintu_air)]\n",
    "new_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Populate the Tanggal and Waktu columns\n",
    "new_df['Tanggal'] = np.repeat(dates, num_times)\n",
    "new_df['Waktu'] = times * num_dates\n",
    "\n",
    "# Extract the measurements from the original DataFrame\n",
    "# Dropping the first row as it is a duplicate header\n",
    "original_data = df.iloc[1:, 1:].reset_index(drop=True)\n",
    "pintu_air_columns = original_data.columns[:num_pintu_air].tolist()\n",
    "\n",
    "# Repeat the original data for the entire year\n",
    "repeated_data = np.tile(original_data[pintu_air_columns].values, (num_dates, 1))\n",
    "\n",
    "# Adjust the size to fit exactly into the new DataFrame\n",
    "adjusted_data = repeated_data[:total_rows, :num_pintu_air]\n",
    "\n",
    "# Assign the adjusted data to the new DataFrame\n",
    "for i, col in enumerate(new_df.columns[2:]):\n",
    "    new_df[col] = adjusted_data[:, i]\n",
    "\n",
    "# Save the new DataFrame to a new Excel file\n",
    "output_file_path = 'Reformatted_TMA2023.xlsx'  # ganti dengan path file output yang diinginkan\n",
    "new_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f'File telah disimpan di: {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 17504 entries, Pintu Air to Tanggal\n",
      "Data columns (total 12 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       27 non-null     object\n",
      " 1   1       27 non-null     object\n",
      " 2   2       27 non-null     object\n",
      " 3   3       27 non-null     object\n",
      " 4   4       27 non-null     object\n",
      " 5   5       27 non-null     object\n",
      " 6   6       27 non-null     object\n",
      " 7   7       27 non-null     object\n",
      " 8   8       27 non-null     object\n",
      " 9   9       27 non-null     object\n",
      " 10  10      27 non-null     object\n",
      " 11  11      27 non-null     object\n",
      "dtypes: object(12)\n",
      "memory usage: 1.7+ MB\n",
      "None\n",
      "Selesai Ditransformasi!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"TMA20232024.xlsx\")\n",
    "\n",
    "# Jumlah baris per blok\n",
    "n = 12\n",
    "\n",
    "# List untuk menyimpan blok yang ditranspos\n",
    "transposed_blocks = []\n",
    "\n",
    "# Memproses setiap blok 13 baris\n",
    "for i in range(0, len(df), n):\n",
    "    # Memilih blok 13 baris\n",
    "    block = df.iloc[i:i+n]\n",
    "    \n",
    "    # Mentranpos blok\n",
    "    transposed_block = block.T\n",
    "    \n",
    "    # Menyimpan blok yang ditranspos\n",
    "    transposed_blocks.append(transposed_block)\n",
    "\n",
    "# Menggabungkan semua blok yang ditranspos ke bawah\n",
    "result = pd.concat(transposed_blocks)\n",
    "#result = transposed_df.iloc[:,0:12]\n",
    "\n",
    "# Mengatur ulang indeks\n",
    "#result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(result.iloc[:,0:12].info())\n",
    "\n",
    "\n",
    "# # Reset index agar index dari 0 hingga akhir\n",
    "# transposed_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "result.to_excel(\"HasilTransformasi-Baru.xlsx\", index=True)\n",
    "print(\"Selesai Ditransformasi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0           1              2           3                4   \\\n",
      "0  Bendung Katulampa   Pos Depok  Manggarai BKB   PA. Karet  Pos Krukut Hulu   \n",
      "1         40 cm (MT)  115 cm (T)     710 cm (T)  350 cm (T)        60 cm (T)   \n",
      "2         40 cm (MT)  115 cm (T)     715 cm (T)  350 cm (T)        60 cm (T)   \n",
      "3         40 cm (MT)  115 cm (T)     715 cm (T)  350 cm (T)        60 cm (T)   \n",
      "4         40 cm (MT)  120 cm (T)     710 cm (T)  340 cm (T)        60 cm (T)   \n",
      "\n",
      "                 5               6            7                  8   \\\n",
      "0  Pos Pesanggrahan  Pos Angke Hulu  Waduk Pluit  Pasar Ikan - Laut   \n",
      "1         90 cm (T)      120 cm (T)  -175 cm (T)         155 cm (M)   \n",
      "2         90 cm (T)      120 cm (T)  -175 cm (T)         150 cm (T)   \n",
      "3         90 cm (T)      120 cm (T)  -160 cm (T)         147 cm (T)   \n",
      "4         90 cm (T)      120 cm (T)  -160 cm (M)         150 cm (T)   \n",
      "\n",
      "                  9                10           11  \n",
      "0  Pos Cipinang Hulu  Pos Sunter Hulu  Pulo Gadung  \n",
      "1         110 cm (T)        70 cm (T)   330 cm (T)  \n",
      "2         110 cm (T)        70 cm (T)   330 cm (T)  \n",
      "3         110 cm (T)        70 cm (T)   330 cm (T)  \n",
      "4         110 cm (T)        70 cm (T)   330 cm (T)  \n"
     ]
    }
   ],
   "source": [
    "print(transposed_df.iloc[:,0:12].head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
